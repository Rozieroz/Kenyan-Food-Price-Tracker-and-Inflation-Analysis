{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbe9bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, max as spark_max, avg, date_format, trunc\n",
    "from pyspark.sql.types import DoubleType\n",
    "import requests\n",
    "import io   # to read the CSV text as an in-memory stream\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.functions import trim, col, regexp_extract, when, round\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "from pyspark.sql.window import Window #to use in date dimension for arrangement order\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee808b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_url = \"https://data.humdata.org/dataset/e0d3fba6-f9a2-45d7-b949-140c455197ff/resource/517ee1bf-2437-4f8c-aa1b-cb9925b9d437/download/wfp_food_prices_ken.csv\"\n",
    "response = requests.get(csv_url)\n",
    "data = response.content.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be759a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 13:51:00 WARN Utils: Your hostname, kali resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlan0)\n",
      "25/07/27 13:51:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/rozie/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/rozie/.ivy2/cache\n",
      "The jars for the packages stored in: /home/rozie/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-83d4251e-674a-439f-a31d-74f431e1d319;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 271ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-83d4251e-674a-439f-a31d-74f431e1d319\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/11ms)\n",
      "25/07/27 13:51:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "       .appName(\"KenyaFoodPrices\") \\\n",
    "       .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "# Download the CSV file as text from the URL by reading remote CSV into memory\n",
    "csv_url = \"https://data.humdata.org/dataset/e0d3fba6-f9a2-45d7-b949-140c455197ff/resource/517ee1bf-2437-4f8c-aa1b-cb9925b9d437/download/wfp_food_prices_ken.csv\"\n",
    "\n",
    "def fetch_and_load_csv():\n",
    "       response = requests.get(csv_url)\n",
    "       data = response.content.decode('utf-8')\n",
    "\n",
    "       # Load into spark df\n",
    "       \n",
    "       lines = data.splitlines()\n",
    "       header = lines[0]\n",
    "       data_lines = lines[1:]\n",
    "\n",
    "       rdd = spark.sparkContext.parallelize(data_lines)\n",
    "       df = spark.read.csv(rdd, header=True, inferSchema=True)\n",
    "\n",
    "       \n",
    "       return df\n",
    "\n",
    "df = fetch_and_load_csv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2d6c2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 13212, Unique rows: 13212, Duplicates: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check for duplicated data\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "total_count = df.count()\n",
    "unique_count = df.dropDuplicates().count()\n",
    "duplicates = total_count - unique_count\n",
    "print(f\"Total rows: {total_count}, Unique rows: {unique_count}, Duplicates: {duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76af97ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#date': 0, '#adm1+name': 45, '#adm2+name': 45, '#loc+market+name': 0, '#loc+market+code': 0, '#geo+lat': 45, '#geo+lon': 45, '#item+type': 0, '#item+name': 0, '#item+code': 0, '#item+unit': 0, '#item+price+flag': 0, '#item+price+type': 0, '#currency+code': 0, '#value': 0, '#value+usd': 0}\n"
     ]
    }
   ],
   "source": [
    "# check for null values\n",
    "null_dict = {c: df.filter(col(c).isNull()).count() for c in df.columns}\n",
    "print(null_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47eca608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+--------------------+--------+---------+------------------+--------------------+----------+-------+--------+---------+--------------+\n",
      "|      date|     region|   district|         market_name|latitude|longitude|         item_type|           commodity|price_type|  price|quantity|base_unit|price_per_unit|\n",
      "+----------+-----------+-----------+--------------------+--------+---------+------------------+--------------------+----------+-------+--------+---------+--------------+\n",
      "|2006-01-15|      Coast|    Mombasa|             Mombasa|   -4.05|    39.67|   pulses and nuts|         Beans (dry)| Wholesale| 3246.0|    90.0|       KG|         36.07|\n",
      "|2006-01-15|    Eastern|      Kitui|               Kitui|   -1.37|    38.02|cereals and tubers|       Maize (white)|    Retail|   17.0|     1.0|       KG|          17.0|\n",
      "|2006-01-15|    Eastern|      Kitui|               Kitui|   -1.37|    38.02|   pulses and nuts|         Beans (dry)|    Retail|   39.0|     1.0|       KG|          39.0|\n",
      "|2006-01-15|    Eastern|   Marsabit|            Marsabit|    2.33|    37.98|cereals and tubers|       Maize (white)|    Retail|   21.0|     1.0|       KG|          21.0|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|cereals and tubers|               Bread|    Retail|   26.0|     0.4|       KG|          65.0|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|cereals and tubers|               Maize| Wholesale|  15.48|     1.0|       KG|         15.48|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|cereals and tubers|       Maize (white)| Wholesale| 1399.0|    90.0|       KG|         15.54|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|cereals and tubers|    Potatoes (Irish)| Wholesale| 664.43|    50.0|       KG|         13.29|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|    milk and dairy|Milk (cow, pasteu...|    Retail|   22.0|     0.5|        L|          44.0|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|   pulses and nuts|               Beans| Wholesale|  42.31|     1.0|       KG|         42.31|\n",
      "|2006-01-15|     Nyanza|     Kisumu|              Kisumu|    -0.1|    34.75|cereals and tubers|               Maize| Wholesale|  14.84|     1.0|       KG|         14.84|\n",
      "|2006-01-15|     Nyanza|     Kisumu|              Kisumu|    -0.1|    34.75|cereals and tubers|    Potatoes (Irish)| Wholesale|1074.07|    50.0|       KG|         21.48|\n",
      "|2006-01-15|Rift Valley|    Turkana|    Lodwar (Turkana)|    3.12|     35.6|cereals and tubers|       Maize (white)|    Retail|   26.0|     1.0|       KG|          26.0|\n",
      "|2006-01-15|Rift Valley|Uasin Gishu|Eldoret town (Uas...|    0.52|    35.28|cereals and tubers|               Maize| Wholesale|  12.96|     1.0|       KG|         12.96|\n",
      "|2006-01-15|Rift Valley|Uasin Gishu|Eldoret town (Uas...|    0.52|    35.28|cereals and tubers|       Maize (white)| Wholesale| 1192.0|    90.0|       KG|         13.24|\n",
      "|2006-01-15|Rift Valley|Uasin Gishu|Eldoret town (Uas...|    0.52|    35.28|   pulses and nuts|               Beans| Wholesale|  45.85|     1.0|       KG|         45.85|\n",
      "|2006-01-15|Rift Valley|Uasin Gishu|Eldoret town (Uas...|    0.52|    35.28|   pulses and nuts|         Beans (dry)| Wholesale| 2799.0|    90.0|       KG|          31.1|\n",
      "|2006-02-15|      Coast|    Mombasa|             Mombasa|   -4.05|    39.67|cereals and tubers|               Maize| Wholesale|  16.41|     1.0|       KG|         16.41|\n",
      "|2006-02-15|      Coast|    Mombasa|             Mombasa|   -4.05|    39.67|   pulses and nuts|               Beans| Wholesale|  39.48|     1.0|       KG|         39.48|\n",
      "|2006-02-15|      Coast|    Mombasa|             Mombasa|   -4.05|    39.67|   pulses and nuts|         Beans (dry)| Wholesale| 3400.0|    90.0|       KG|         37.78|\n",
      "+----------+-----------+-----------+--------------------+--------+---------+------------------+--------------------+----------+-------+--------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_and_transform(df):\n",
    "\n",
    "       # rename columns for ease\n",
    "       df = df.withColumnRenamed(\"#item+name\", \"commodity\") \\\n",
    "              .withColumnRenamed(\"#date\", \"date\") \\\n",
    "              .withColumnRenamed(\"#value\", \"price\") \\\n",
    "              .withColumnRenamed(\"#item+unit\", \"unit\") \\\n",
    "              .withColumnRenamed(\"#adm1+name\", \"region\") \\\n",
    "              .withColumnRenamed(\"#adm2+name\", \"district\") \\\n",
    "              .withColumnRenamed(\"#loc+market+name\", \"market_name\") \\\n",
    "              .withColumnRenamed(\"#geo+lat\", \"latitude\") \\\n",
    "              .withColumnRenamed(\"#geo+lon\", \"longitude\") \\\n",
    "              .withColumnRenamed(\"#item+price+type\", \"price_type\") \\\n",
    "              .withColumnRenamed(\"#item+type\", \"item_type\") \n",
    "\n",
    "\n",
    "\n",
    "       # convert date column and cast price to double\n",
    "       df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "       df = df.withColumn(\"price\", col(\"price\").cast(DoubleType()))\n",
    "\n",
    "\n",
    "       # get latest date and compute start date for 9 months back\n",
    "       \"\"\"latest_date = df.select(spark_max(\"date\")).first()[0]\n",
    "\n",
    "       start_date = latest_date.replace(day=1) - relativedelta(months=9)\n",
    "       df_filtered = df.filter((col(\"date\") > start_date) & (col(\"date\") <= latest_date))\n",
    "\"\"\"\n",
    "\n",
    "       # extract quantity and the unit of measure\n",
    "       df_extract = df \\\n",
    "              .withColumn(\"unit\", trim(col(\"unit\"))) \\\n",
    "              .withColumn(\"quantity\", regexp_extract(col(\"unit\"), r\"(\\d+)\", 1).cast(DoubleType())) \\\n",
    "              .withColumn(\"base_unit\", regexp_extract(col(\"unit\"), r\"[a-zA-Z]+\", 0))\n",
    "       \n",
    "\n",
    "       # fill the nulls left after extracting the base unit\n",
    "       df_clean = df_extract.withColumn(\"quantity\", when(col(\"quantity\").isNull(), 1.0).otherwise(col(\"quantity\")))\n",
    "\n",
    "\n",
    "       # clean for columns with less than 1kg or 1l eg milk, salt to show in kg or l\n",
    "       df_standard = df_clean.withColumn(\"quantity\", \n",
    "                                   when(col(\"base_unit\") == \"G\", col(\"quantity\") / 1000)\n",
    "                                   .when(col(\"base_unit\") == \"ML\", col(\"quantity\") / 1000)\n",
    "                                   .otherwise(col(\"quantity\"))\n",
    "                                          ) \\\n",
    "                            .withColumn(\"base_unit\",        #standardize to have a common unit\n",
    "                            when(col(\"base_unit\") == \"G\", \"KG\")\n",
    "                        .when(col(\"base_unit\") == \"ML\", \"L\")\n",
    "                        .when(col(\"base_unit\") == \"Unit\", \"PIECE\")\n",
    "                        .otherwise(col(\"base_unit\"))\n",
    "                        )\n",
    "       # get the price per unit or kg- to make it easier for those with more than\n",
    "       df_cleaned= df_standard.withColumn(\"price_per_unit\", round((col(\"price\") / col(\"quantity\")).cast(DoubleType()), 2))\n",
    "       # df_cleaned.show()\n",
    "       \n",
    "\n",
    "       # drop most irrelevant cols\n",
    "       cols_to_drop = [\n",
    "              \"#item+code\", \"#loc+market+code\",\n",
    "              \"#item+price+flag\", \"#currency+code\", \"#value+usd\",\n",
    "              \"unit\", \"measure\"\n",
    "              ]\n",
    "\n",
    "       clean_cols = df_cleaned.drop(*cols_to_drop)\n",
    "\n",
    "       # drop Hola (Tana River) due to null region, district, long, lat\n",
    "\n",
    "       df_final = clean_cols.filter(\n",
    "              col(\"region\").isNotNull() & \n",
    "              col(\"commodity\").isNotNull() & \n",
    "              col(\"price\").isNotNull()\n",
    "       )\n",
    "       df_final.show()\n",
    "\n",
    "       return df_final\n",
    "       # return df_filtered\n",
    "\n",
    "data = clean_and_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c4d19",
   "metadata": {},
   "source": [
    "## Build fact and dimension tables with star schema\n",
    "\n",
    "The dimensional tables allow  to cleanly separate these:\n",
    "\n",
    "dim_product → standardize commodity + type (for grouping by commodity or category)\n",
    "\n",
    "dim_location → organize region, district, market, lat/lon\n",
    "    -geographic analysis: prices by region, county, or market\n",
    "\n",
    "dim_date → enable time-based grouping\n",
    "    - Enable time-series slicing\n",
    "\n",
    "dim_price_type → capture pricing methodo ( filter by retail, wholesale as they may behave differently)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abb72829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, year, month, dayofmonth\n",
    "\n",
    "# def build_product_dimension(df):\n",
    "#     return df.select(\"commodity\", \"item_type\").dropDuplicates() \\\n",
    "#              .withColumn(\"product_id\", monotonically_increasing_id())\n",
    "\n",
    "def build_product_dimension(df):\n",
    "    deduped = df.select(\"commodity\", \"item_type\").dropDuplicates()\n",
    "    with_id = deduped.withColumn(\"product_id\", monotonically_increasing_id())\n",
    "    return with_id.select(\"product_id\", \"commodity\", \"item_type\")\n",
    "\n",
    "def build_location_dim(df):\n",
    "    deduped = df.select(\"region\", \"district\", \"market_name\", \"latitude\", \"longitude\").dropDuplicates()\n",
    "    with_id = deduped.withColumn(\"location_id\", monotonically_increasing_id())\n",
    "    return with_id.select(\"location_id\", \"region\", \"district\", \"market_name\", \"latitude\", \"longitude\")\n",
    "\n",
    "def build_date_dim(df):\n",
    "    deduped = df.select(\"date\").dropDuplicates()\n",
    "    \n",
    "    enriched = deduped \\\n",
    "        .withColumn(\"year\", year(\"date\")) \\\n",
    "        .withColumn(\"month\", month(\"date\")) \\\n",
    "        .withColumn(\"day\", dayofmonth(\"date\"))\n",
    "\n",
    "    window = Window.partitionBy(\"year\").orderBy(\"date\")\n",
    "    with_id = enriched.withColumn(\"date_id\", row_number().over(window) - 1)  # zero-based IDs\n",
    "\n",
    "    return with_id.select(\"date_id\", \"date\", \"year\", \"month\", \"day\")\n",
    "\n",
    "def build_price_type_dim(df):\n",
    "    deduped = df.select(\"price_type\").dropDuplicates()\n",
    "    with_id = deduped.withColumn(\"price_type_id\", monotonically_increasing_id())\n",
    "    return with_id.select(\"price_type_id\", \"price_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    fact price table that joins all the dimension tables\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_fact_table(df_final, product_dim, location_dim, date_dim, price_type_dim):\n",
    "    fact = df_final.join(product_dim,\n",
    "                   (df_final.commodity == product_dim.commodity) & \n",
    "                   (df_final.item_type == product_dim.item_type),\n",
    "                   \"left\") \\\n",
    "             .join(location_dim,\n",
    "                   (df_final.region == location_dim.region) &\n",
    "                   (df_final.district == location_dim.district) &\n",
    "                   (df_final.market_name == location_dim.market_name) &\n",
    "                   (df_final.latitude == location_dim.latitude) &\n",
    "                   (df_final.longitude == location_dim.longitude),\n",
    "                   \"left\") \\\n",
    "             .join(date_dim, \"date\", \"left\") \\\n",
    "             .join(price_type_dim, \"price_type\", \"left\")\n",
    "    \n",
    "    return fact.select(\n",
    "        \"product_id\", \"location_id\", \"date_id\", \"price_type_id\",\n",
    "        \"price_per_unit\", \"quantity\"  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e87debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to db\n",
    "def write_to_postgres(df, table_name: str, schema = \"food_prices\", mode: str = \"overwrite\"):\n",
    "\n",
    "    user = os.getenv(\"DB_USER\")\n",
    "    password = os.getenv(\"PASSWORD\")\n",
    "    host = os.getenv(\"HOST\")  \n",
    "    port = os.getenv(\"PORT\")  \n",
    "    database = os.getenv(\"DB\")\n",
    "\n",
    "    jdbc_url = f\"jdbc:postgresql://{host}:{port}/{database}?sslmode=require&currentSchema={schema}\"\n",
    "    properties = {\n",
    "        \"user\": user,\n",
    "        \"password\": password,\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    full_table_name = f\"{schema}.{table_name}\"  # schema.table so tables are stored in the correct schema\n",
    "\n",
    "    print(f\"Connecting to: {host}:{port}/{database} as {user}\")\n",
    "    print(f\"Writing to table: {full_table_name}\")\n",
    "\n",
    "    df.write.jdbc(url=jdbc_url, table=full_table_name, mode=mode, properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "480eff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+--------------------+--------+---------+------------------+--------------------+----------+-------+--------+---------+--------------+\n",
      "|      date|     region|   district|         market_name|latitude|longitude|         item_type|           commodity|price_type|  price|quantity|base_unit|price_per_unit|\n",
      "+----------+-----------+-----------+--------------------+--------+---------+------------------+--------------------+----------+-------+--------+---------+--------------+\n",
      "|2006-01-15|      Coast|    Mombasa|             Mombasa|   -4.05|    39.67|   pulses and nuts|         Beans (dry)| Wholesale| 3246.0|    90.0|       KG|         36.07|\n",
      "|2006-01-15|    Eastern|      Kitui|               Kitui|   -1.37|    38.02|cereals and tubers|       Maize (white)|    Retail|   17.0|     1.0|       KG|          17.0|\n",
      "|2006-01-15|    Eastern|      Kitui|               Kitui|   -1.37|    38.02|   pulses and nuts|         Beans (dry)|    Retail|   39.0|     1.0|       KG|          39.0|\n",
      "|2006-01-15|    Eastern|   Marsabit|            Marsabit|    2.33|    37.98|cereals and tubers|       Maize (white)|    Retail|   21.0|     1.0|       KG|          21.0|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|cereals and tubers|               Bread|    Retail|   26.0|     0.4|       KG|          65.0|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|cereals and tubers|               Maize| Wholesale|  15.48|     1.0|       KG|         15.48|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|cereals and tubers|       Maize (white)| Wholesale| 1399.0|    90.0|       KG|         15.54|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|cereals and tubers|    Potatoes (Irish)| Wholesale| 664.43|    50.0|       KG|         13.29|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|    milk and dairy|Milk (cow, pasteu...|    Retail|   22.0|     0.5|        L|          44.0|\n",
      "|2006-01-15|    Nairobi|    Nairobi|             Nairobi|   -1.28|    36.82|   pulses and nuts|               Beans| Wholesale|  42.31|     1.0|       KG|         42.31|\n",
      "|2006-01-15|     Nyanza|     Kisumu|              Kisumu|    -0.1|    34.75|cereals and tubers|               Maize| Wholesale|  14.84|     1.0|       KG|         14.84|\n",
      "|2006-01-15|     Nyanza|     Kisumu|              Kisumu|    -0.1|    34.75|cereals and tubers|    Potatoes (Irish)| Wholesale|1074.07|    50.0|       KG|         21.48|\n",
      "|2006-01-15|Rift Valley|    Turkana|    Lodwar (Turkana)|    3.12|     35.6|cereals and tubers|       Maize (white)|    Retail|   26.0|     1.0|       KG|          26.0|\n",
      "|2006-01-15|Rift Valley|Uasin Gishu|Eldoret town (Uas...|    0.52|    35.28|cereals and tubers|               Maize| Wholesale|  12.96|     1.0|       KG|         12.96|\n",
      "|2006-01-15|Rift Valley|Uasin Gishu|Eldoret town (Uas...|    0.52|    35.28|cereals and tubers|       Maize (white)| Wholesale| 1192.0|    90.0|       KG|         13.24|\n",
      "|2006-01-15|Rift Valley|Uasin Gishu|Eldoret town (Uas...|    0.52|    35.28|   pulses and nuts|               Beans| Wholesale|  45.85|     1.0|       KG|         45.85|\n",
      "|2006-01-15|Rift Valley|Uasin Gishu|Eldoret town (Uas...|    0.52|    35.28|   pulses and nuts|         Beans (dry)| Wholesale| 2799.0|    90.0|       KG|          31.1|\n",
      "|2006-02-15|      Coast|    Mombasa|             Mombasa|   -4.05|    39.67|cereals and tubers|               Maize| Wholesale|  16.41|     1.0|       KG|         16.41|\n",
      "|2006-02-15|      Coast|    Mombasa|             Mombasa|   -4.05|    39.67|   pulses and nuts|               Beans| Wholesale|  39.48|     1.0|       KG|         39.48|\n",
      "|2006-02-15|      Coast|    Mombasa|             Mombasa|   -4.05|    39.67|   pulses and nuts|         Beans (dry)| Wholesale| 3400.0|    90.0|       KG|         37.78|\n",
      "+----------+-----------+-----------+--------------------+--------+---------+------------------+--------------------+----------+-------+--------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Connecting to: pg-ecfa71b-rosewabere4-99fd.b.aivencloud.com:12410/defaultdb as avnadmin\n",
      "Writing to table: food_prices.product_dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to: pg-ecfa71b-rosewabere4-99fd.b.aivencloud.com:12410/defaultdb as avnadmin\n",
      "Writing to table: food_prices.location_dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to: pg-ecfa71b-rosewabere4-99fd.b.aivencloud.com:12410/defaultdb as avnadmin\n",
      "Writing to table: food_prices.date_dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to: pg-ecfa71b-rosewabere4-99fd.b.aivencloud.com:12410/defaultdb as avnadmin\n",
      "Writing to table: food_prices.price_type_dim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to: pg-ecfa71b-rosewabere4-99fd.b.aivencloud.com:12410/defaultdb as avnadmin\n",
      "Writing to table: food_prices.fact_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 13:52:44 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "\n",
    "    # 1. extract and load data\n",
    "    df = fetch_and_load_csv()\n",
    "\n",
    "    # 2. clean and transform data\n",
    "    data = clean_and_transform(df)\n",
    "\n",
    "    # 3. dimension and fact tables\n",
    "    \n",
    "    dim_product = build_product_dimension(data)\n",
    "    dim_location = build_location_dim(data)\n",
    "    dim_date = build_date_dim(data)\n",
    "    dim_price_type = build_price_type_dim(data)\n",
    "\n",
    "    fact_table = build_fact_table(data, dim_product, dim_location, dim_date, dim_price_type)\n",
    "\n",
    "    # 4. store to db\n",
    "    \n",
    "    write_to_postgres(dim_product, \"product_dim\")\n",
    "    write_to_postgres(dim_location, \"location_dim\")\n",
    "    write_to_postgres(dim_date, \"date_dim\")\n",
    "    write_to_postgres(dim_price_type, \"price_type_dim\")\n",
    "    write_to_postgres(fact_table, \"fact_table\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f20db208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: long (nullable = false)\n",
      " |-- region: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- market_name: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_location.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
